{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49945b68",
   "metadata": {},
   "source": [
    "**Official implementation of paper \"Token-based Decision Criteria Are Suboptimal in In-context Learning\" (NAACL 2025)**:\n",
    "\n",
    "### Source code for the Analysis in Sec. 5.2.\n",
    "\n",
    "This repository contains the source code for the analysis in Sec. 5.2 (a part) of the paper \"Token-based Decision Criteria Are Suboptimal in In-context Learning\" (NAACL 2025).\n",
    "\n",
    "Mainly to calculate the inter-category distance.\n",
    "\n",
    "Used in experiments of:\n",
    "\n",
    "1. The blue curve in the Fig. 12.\n",
    "\n",
    "Author: Hakaze Cho, yfzhao@jaist.ac.jp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15423221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "huggingface_model_name = \"facebook/opt-2.7b\"\n",
    "huggingface_token = \"API_TOKEN\"\n",
    "quantization = False\n",
    "\n",
    "dataset_name = \"SemEvalR\" # Alternative: \"SemEvalR\", \"SemEvalL\", \"poem_sentiment\", \"TEE\", \"TEH\", \"TES\", \"FP\", \"AGNews\", \"MR\", \"hate_speech\"\n",
    "\n",
    "k = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1fb8913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries, and nessessary definitions\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"hidden_calibration_released\") # Replace with the path from the working directory to the root of this project. If the working directory is already the root of the project, this line is not needed.\n",
    "\n",
    "import util.prompting as prompting\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "import util.dataset_loader as dataset_loader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def softmax(x):\n",
    "    f_x = np.exp(x) / np.sum(np.exp(x))\n",
    "    return f_x\n",
    "\n",
    "dataset_name_to_class = {\n",
    "    \"SemEvalR\": dataset_loader.SemEval2014_Restaurants,\n",
    "    \"SemEvalL\": dataset_loader.SemEval2014_Laptops,\n",
    "    \"poem_sentiment\": dataset_loader.poem_sentiment,\n",
    "    \"TEE\": dataset_loader.tweet_eval_emotion,\n",
    "    \"TEH\": dataset_loader.tweet_eval_hate,\n",
    "    \"TES\": dataset_loader.tweet_eval_sentiment,\n",
    "    \"FP\": dataset_loader.financial_phrasebank,\n",
    "    \"AGNews\": dataset_loader.agnews,\n",
    "    \"MR\": dataset_loader.rooten_tomato,\n",
    "    \"hate_speech\": dataset_loader.hate_speech18,\n",
    "}\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.rc('font',family='Times New Roman')\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f850c1bd-1745-4414-8789-8d8e20fff497",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load model and tokenizer from Huggingface\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(huggingface_model_name, token = huggingface_token)\n",
    "if quantization:\n",
    "    model = AutoModelForCausalLM.from_pretrained(huggingface_model_name, token = huggingface_token, quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    ))\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(huggingface_model_name, token = huggingface_token).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ea54351-9308-427f-828c-df0e9ec30200",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the training and testing data\n",
    "\n",
    "test_data = dataset_name_to_class[dataset_name]().default_testing_division()\n",
    "if dataset_name == \"AGNews\":\n",
    "    test_data.cut_by_length(226)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f416a1c3",
   "metadata": {},
   "source": [
    "**Calculate the hidden states of the text examples.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a49c445-be3f-48c2-abd2-ffe56285e51d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inference the hidden states of the testing data\n",
    "\n",
    "label_indexed_by_test_samples = []\n",
    "last_hidden_state_indexed_by_test_samples = []\n",
    "\n",
    "torch.no_grad()\n",
    "for i in tqdm(range(test_data.get_max())):\n",
    "    torch.cuda.empty_cache()\n",
    "    prpt = prompting.default_prompting(test_data, k, query_index=i)\n",
    "    tknzd_data = tokenizer(prpt[0], return_tensors=\"pt\").input_ids.cuda()\n",
    "    result = model(tknzd_data, output_hidden_states = True)\n",
    "    hidden_state = result.hidden_states[-1][-1][-1].detach().cpu().numpy()\n",
    "    last_hidden_state_indexed_by_test_samples.append(hidden_state)\n",
    "    tkized_label_space = []\n",
    "    label_indexed_by_test_samples.append(test_data.label_space.index(prpt[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6384ad48-a4ad-44c9-8579-f1906756cc1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the centroid of each label\n",
    "\n",
    "hidden_state_list_indexed_by_labels = []\n",
    "\n",
    "for label in test_data.label_space:\n",
    "    hidden_state_list_indexed_by_labels.append([])\n",
    "    \n",
    "for i in range(len(label_indexed_by_test_samples)):\n",
    "    hidden_state_list_indexed_by_labels[label_indexed_by_test_samples[i]].append(last_hidden_state_indexed_by_test_samples[i])\n",
    "    \n",
    "hidden_state_centroid_indexed_by_labels = []\n",
    "for lists in hidden_state_list_indexed_by_labels:\n",
    "    hidden_state_centroid_indexed_by_labels.append(np.mean(lists, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e2fabf4-dd59-4aec-96f2-44a06df4adfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the averaged distance between the centroids of each label\n",
    "\n",
    "distance_in_pair = []\n",
    "distance_list = []\n",
    "\n",
    "for i in range(len(hidden_state_centroid_indexed_by_labels)):\n",
    "    temp = []\n",
    "    for j in range(len(hidden_state_centroid_indexed_by_labels)):\n",
    "        distance = spatial.distance.euclidean(hidden_state_centroid_indexed_by_labels[i], hidden_state_centroid_indexed_by_labels[j])\n",
    "        temp.append(distance)\n",
    "        if i != j:\n",
    "            distance_list.append(distance)\n",
    "    distance_in_pair.append(temp) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fc3c51",
   "metadata": {},
   "source": [
    "**Result output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9008f157-16b1-4f42-a5d5-c45e0f8e9537",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Result report on \" + dataset_name + \" dataset, metric: Euclidean distance between centroids of each label\\n\")\n",
    "\n",
    "print(distance_in_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206e3dae-ace2-47f0-b046-bb81c18ecd4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Result report on \" + dataset_name + \" dataset, metric: averaged Euclidean distance among centroids of every label\\n\")\n",
    "\n",
    "print(np.mean(distance_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
